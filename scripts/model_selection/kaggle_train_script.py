"""
Kaggle SAM ViT-L LoRA å¾®è°ƒè„šæœ¬ (ä¿®å¤ç‰ˆV2)
================================
å˜æ›´è¯´æ˜ï¼š
1. åˆ‡æ¢æ¨¡å‹ä¸º **ViT-L** (Large, 308Må‚æ•°) - è§£å†³ViT-H (Huge)åœ¨P100ä¸Šçš„æ˜¾å­˜ä¸è¶³(OOM)é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒæ¯”ViT-Bå¼ºå¾—å¤šçš„æ€§èƒ½ã€‚
2. ä¿æŒ **1024x1024** é«˜åˆ†è¾¨ç‡ã€‚
3. å¦‚æœä¾ç„¶OOMï¼Œè¯·åœ¨é…ç½®ä¸­å°† `img_size` æ”¹ä¸º 768 æˆ– 512ã€‚

ä½¿ç”¨è¯´æ˜ï¼š
1. å¤åˆ¶æœ¬ä»£ç åˆ°Kaggle Notebookè¿è¡Œã€‚
2. ç¡®ä¿ 'kaggle_dataset_fixed.zip' å·²ä¸Šä¼ ã€‚
"""

import os
import sys
import json
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

# ----------------- 1. ç¯å¢ƒå®‰è£… -----------------
print("æ­£åœ¨å®‰è£…ä¾èµ–...")
os.system("pip install segment-anything peft -q")

from segment_anything import sam_model_registry
from segment_anything.modeling import Sam
from peft import LoraConfig, get_peft_model

# ----------------- 2. é…ç½® -----------------
CONFIG = {
    # Kaggleæ•°æ®è·¯å¾„
    'image_dir': '/kaggle/input/i-need-u/å•æ™¶å›¾åƒ_png', 
    'label_dir': '/kaggle/input/i-need-u/æ•°æ®',
    'img_size': 1024, # å°è¯•ä¿æŒé«˜åˆ†è¾¨ç‡
    'epochs': 100,
    'lr': 1e-4,
    'batch_size': 1,
    'accumulation_steps': 4, # æ¢¯åº¦ç´¯ç§¯
    'lora_r': 16,
    'lora_alpha': 32,
    'device': torch.device("cuda" if torch.cuda.is_available() else "cpu")
}

# è·¯å¾„è‡ªåŠ¨æ£€æŸ¥
if not os.path.exists(CONFIG['image_dir']):
    if os.path.exists("./images"): # å…¼å®¹æœ¬åœ°/è§£å‹è·¯å¾„
        CONFIG['image_dir'] = "./images"
        CONFIG['label_dir'] = "./labels"
    else:
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ•°æ®ç›®å½• {CONFIG['image_dir']}")

print(f"ä½¿ç”¨è®¾å¤‡: {CONFIG['device']}")

# ----------------- 3. ä¸‹è½½æƒé‡ (ViT-L) -----------------
# åˆ‡æ¢åˆ°ViT-L
WEIGHT_PATH = "sam_vit_l_0b3195.pth"
if not os.path.exists(WEIGHT_PATH):
    print("æ­£åœ¨ä¸‹è½½SAM ViT-Læƒé‡ (1.2GB)...")
    os.system("wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth")
    print("ä¸‹è½½å®Œæˆã€‚")

# ----------------- 4. æ•°æ®é›†å®šä¹‰ -----------------
class GammaPrimeDataset(Dataset):
    def __init__(self, image_dir, label_dir, img_size=1024):
        self.image_dir = Path(image_dir)
        self.label_dir = Path(label_dir)
        self.img_size = img_size
        self.json_files = list(self.label_dir.glob("*.json"))
        print(f"æ‰¾åˆ° {len(self.json_files)} ä¸ªæ ·æœ¬")
        
    def _detect_crop_height(self, img):
        h, w = img.shape[:2]
        bottom_h = int(h * 0.75)
        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape)==3 else img
        bottom_part = img_gray[bottom_h:, :]
        edges = cv2.Canny(bottom_part, 50, 150)
        edge_sum = np.sum(edges, axis=1)
        candidates = np.where(edge_sum > w * 0.5 * 255)[0]
        if len(candidates) > 0:
            return bottom_h + candidates[0]
        return int(h * 0.85)
    
    def __len__(self):
        return len(self.json_files)
    
    def __getitem__(self, idx):
        json_path = self.json_files[idx]
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        img_stem = json_path.stem
        img_path = list(self.image_dir.glob(f"{img_stem}.*"))[0]
        
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        crop_h = self._detect_crop_height(image)
        h, w = image.shape[:2]
        image = image[:crop_h, :]
        # ç”ŸæˆMask
        mask = np.zeros((crop_h, w), dtype=np.uint8)
        for shape in data.get("shapes", []):
            points = np.array(shape["points"], dtype=np.int32)
            points[:, 1] = np.clip(points[:, 1], 0, crop_h - 1)
            cv2.fillPoly(mask, [points], 1)
        
        # ğŸŸ¢ æ•°æ®å¢å¼º (å¯¹9å¼ å›¾çš„å°æ•°æ®éå¸¸é‡è¦!)
        if np.random.rand() > 0.5: # æ°´å¹³ç¿»è½¬
            image = cv2.flip(image, 1)
            mask = cv2.flip(mask, 1)
        if np.random.rand() > 0.5: # å‚ç›´ç¿»è½¬
            image = cv2.flip(image, 0)
            mask = cv2.flip(mask, 0)
        
        k = np.random.randint(0, 4) # éšæœºæ—‹è½¬ 0, 90, 180, 270åº¦
        if k > 0:
            image = np.rot90(image, k).copy()
            mask = np.rot90(mask, k).copy()
        
        # Resize
        image_resized = cv2.resize(image, (self.img_size, self.img_size))
        mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)
        
        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float()
        mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)
        
        return image_tensor, mask_tensor

# ----------------- 5. æ¨¡å‹å®šä¹‰ (LoRA) -----------------
class SAMLoRAWrapper(nn.Module):
    def __init__(self, sam_model, lora_r=16, lora_alpha=32):
        super().__init__()
        self.sam = sam_model
        
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["qkv"],
            lora_dropout=0.1,
            bias="none",
        )
        self.sam.image_encoder = get_peft_model(self.sam.image_encoder, lora_config)
        
        for param in self.sam.mask_decoder.parameters():
            param.requires_grad = True
        for param in self.sam.prompt_encoder.parameters():
            param.requires_grad = False
            
        trainable = sum(p.numel() for p in self.sam.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.sam.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable/1e6:.2f}M / {total/1e6:.2f}M ({trainable/total*100:.2f}%)")

    def forward(self, images):
        # ğŸŸ¢ å…³é”®ä¿®å¤: æ·»åŠ SAMé¢„å¤„ç† (å½’ä¸€åŒ– + Pad)
        # images: (B, 3, H, W) 0-255
        x = self.sam.preprocess(images)
        image_embeddings = self.sam.image_encoder(x)
        
        batch_size = images.shape[0]
        
        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(
            points=None, boxes=None, masks=None
        )
        sparse_embeddings = sparse_embeddings.expand(batch_size, -1, -1)
        dense_embeddings = dense_embeddings.expand(batch_size, -1, -1, -1)
        
        low_res_masks, _ = self.sam.mask_decoder(
            image_embeddings=image_embeddings,
            image_pe=self.sam.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )
        return low_res_masks

# ----------------- 6. è®­ç»ƒå¾ªç¯ -----------------
def dice_loss(pred, target, smooth=1e-5):
    pred = torch.sigmoid(pred)
    intersection = (pred * target).sum(dim=(2, 3))
    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))
    dice = (2 * intersection + smooth) / (union + smooth)
    return 1 - dice.mean()

def train():
    # åˆ‡æ¢ä¸ºViT-L
    sam = sam_model_registry["vit_l"](checkpoint=WEIGHT_PATH)
    model = SAMLoRAWrapper(sam, lora_r=CONFIG['lora_r'], lora_alpha=CONFIG['lora_alpha'])
    model.to(CONFIG['device'])
    
    dataset = GammaPrimeDataset(CONFIG['image_dir'], CONFIG['label_dir'], img_size=CONFIG['img_size'])
    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=CONFIG['lr'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])
    scaler = torch.amp.GradScaler('cuda')
    
    best_dice = 0
    history = {'loss': [], 'dice': []}
    
    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(CONFIG['epochs']):
        model.train()
        epoch_loss = 0
        epoch_dice = 0
        optimizer.zero_grad()
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")
        for batch_idx, (images, masks) in enumerate(pbar):
            images = images.to(CONFIG['device'])
            masks = masks.to(CONFIG['device'])
            
            with torch.amp.autocast('cuda'):
                preds = model(images)
                loss = dice_loss(preds, masks) / CONFIG['accumulation_steps']
            
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % CONFIG['accumulation_steps'] == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            epoch_loss += loss.item() * CONFIG['accumulation_steps']
            
            with torch.no_grad():
                pred_binary = (torch.sigmoid(preds) > 0.5).float()
                dice = (2 * (pred_binary * masks).sum()) / (pred_binary.sum() + masks.sum() + 1e-8)
                epoch_dice += dice.item()
            
            pbar.set_postfix({'loss': loss.item() * CONFIG['accumulation_steps'], 'dice': dice.item()})
        
        if len(dataloader) % CONFIG['accumulation_steps'] != 0:
             scaler.step(optimizer)
             scaler.update()
             optimizer.zero_grad()
        
        scheduler.step()
        avg_dice = epoch_dice / len(dataloader)
        history['dice'].append(avg_dice)
        history['loss'].append(epoch_loss / len(dataloader))
        
        if avg_dice > best_dice:
            best_dice = avg_dice
            # ä¿å­˜
            model.sam.image_encoder.save_pretrained("sam_lora_encoder")
            torch.save(model.sam.mask_decoder.state_dict(), "sam_decoder.pth")
            torch.save({
                'encoder_lora': model.sam.image_encoder.state_dict(),
                'decoder': model.sam.mask_decoder.state_dict(),
                'best_dice': best_dice
            }, "sam_lora_best.pth")
            print(f"ğŸ”¥ æ–°æœ€ä½³Dice: {best_dice:.4f} (å·²ä¿å­˜)")
            
    plt.figure(figsize=(10, 5))
    plt.plot(history['dice'], label='Dice')
    plt.plot(history['loss'], label='Loss')
    plt.legend()
    plt.title(f"Training History (Best Dice: {best_dice:.4f})")
    plt.savefig("training_curve.png")
    plt.show()
    print("è®­ç»ƒç»“æŸï¼è¯·ä¸‹è½½ 'sam_lora_best.pth' å’Œ 'training_curve.png'")

if __name__ == "__main__":
    try:
        train()
    except Exception as e:
        print(f"å‡ºé”™å•¦: {e}")
        import traceback
        traceback.print_exc()
